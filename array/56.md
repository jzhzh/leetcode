# 56. Merge Intervals

## collections.defaultdict()

When we try to transfer the key-value list to a list dict, we can use `collections.defaultdict()` to implement it.

```python
s=[('yellow',1),('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]
d=defaultdict(list)
for k, v in s:
	d[k].append(v)
a=sorted(d.items())
print(d)
print(a)

'''
defaultdict(, {'blue': [2, 4], 'red': [1], 'yellow': [1, 3]})
[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])]
'''
```

## Inner Functions

1. The enclosing function provides a namespace that is accessible to the inner function, which means that the inner functions can access the variables that exist out of inner functions but in the outer functions.

```python
>>> def outer_func(who):
...     def inner_func():
...         print(f"Hello, {who}")
...     inner_func()
...

>>> outer_func("World!")
Hello, World!
```

   Here `inner_func()` can access variable *who* in the `outer_func()`.

1. Providing Encapsulation.

   A common use case of inner functions arises when you need to protect, or hide, a given function from everything happening outside of it so that the function is totally hidden from the global scope. This kind of behavior is commonly known as encapsulation.

```python
>>> def increment(number):
...     def inner_increment():
...         return number + 1
...     return inner_increment()

>>> increment(10)
11

>>> # Call inner_increment()
>>> inner_increment()
Traceback (most recent call last):
  File "<input>", line 1, in <module>
    inner_increment()
NameError: name 'inner_increment' is not defined
```

1. Building Helper Inner Functions

   Sometimes you have a function that performs the same chunk of code in several places within its body. For example, say you want to write a function to process a CSV file containing information about the Wi-Fi hotspots in New York City. To find the total number of hotspots in New York as well as the company that provides most of them, you create the following script:

```python
# hotspots.py

import csv
from collections import Counter

def process_hotspots(file):
    def most_common_provider(file_obj):
        hotspots = []
        with file_obj as csv_file:
            content = csv.DictReader(csv_file)

            for row in content:
                hotspots.append(row["Provider"])

        counter = Counter(hotspots)
        print(
            f"There are {len(hotspots)} Wi-Fi hotspots in NYC.\n"
            f"{counter.most_common(1)[0][0]} has the most with "
            f"{counter.most_common(1)[0][1]}."
        )

    if isinstance(file, str):
        # Got a string-based filepath
        file_obj = open(file, "r")
        most_common_provider(file_obj)
    else:
        # Got a file object
        most_common_provider(file)
```

   Here, `process_hotspots()` takes file as an argument. The function checks if file is a string-based path to a physical file or a file object. Then it calls the helper inner function `most_common_provider()`. If you run the function, then you get the following output:

```python
>>> from hotspots import process_hotspots

>>> file_obj = open("./NYC_Wi-Fi_Hotspot_Locations.csv", "r")
>>> process_hotspots(file_obj)
There are 3319 Wi-Fi hotspots in NYC.
LinkNYC - Citybridge has the most with 1868.

>>> process_hotspots("./NYC_Wi-Fi_Hotspot_Locations.csv")
There are 3319 Wi-Fi hotspots in NYC.
LinkNYC - Citybridge has the most with 1868.
```

1. Retaining State in a Closure

   A closure causes the inner function to retain the state of its environment when called. The closure isn’t the inner function itself but the inner function along with its enclosing environment.

```python
# powers.py
 
def generate_power(exponent):
    def power(base):
        return base ** exponent
    return power
```

   `generate_power()`, which is a closure factory function. This means that it creates a new closure each time it’s called and then returns it to the caller.Finally it returns power as a function object, without calling it.

   when you call the instance of `power()` returned by `generate_power()`, you’ll see that the function remembers the value of exponent:

```python
>>> from powers import generate_power

>>> raise_two = generate_power(2)
>>> raise_three = generate_power(3)

>>> raise_two(4)
16
>>> raise_two(5)
25

>>> raise_three(4)
64
>>> raise_three(5)
125
```

   You’ll commonly create closures that don’t modify their enclosing state, or closures with a static enclosing state, as you saw in the above examples. However, you can also create closures that modify their enclosing state by using mutable objects, such as dictionaries, sets, or lists.

```python
>>> def mean():
...     sample = []
...     def inner_mean(number):
...         sample.append(number)
...         return sum(sample) / len(sample)
...     return inner_mean

>>> sample_mean = mean()
>>> sample_mean(100)
100.0
>>> sample_mean(105)
102.5
>>> sample_mean(101)
102.0
>>> sample_mean(98)
101.0
```

1. Modifying the Closure State

   Normally, closure variables are completely hidden from the outside world. However, you can provide getter and setter inner functions for them:

```python
>>> def make_point(x, y):
...     def point():
...         print(f"Point({x}, {y})")
...     def get_x():
...         return x
...     def get_y():
...         return y
...     def set_x(value):
...         nonlocal x
...         x = value
...     def set_y(value):
...         nonlocal y
...         y = value
...     # Attach getters and setters
...     point.get_x = get_x
...     point.set_x = set_x
...     point.get_y = get_y
...     point.set_y = set_y
...     return point

>>> point = make_point(1, 2)
>>> point.get_x()
1
>>> point.get_y()
2
>>> point()
Point(1, 2)

>>> point.set_x(42)
>>> point.set_y(7)
>>> point()
Point(42, 7)
```

   Here, `make_point()` returns a closure that represents a point object which is also a function object. Knowing that everything in Python is object, we can regard the function itself as an object which also has attributes. Here point is the object of `point()`. And point has the attribute `get_x` which is the object format of `get_x()`. So when we do instantiation such as `instant = make_point(1, 2)`, `make_point(1, 2)` returns an object point to instant, which means instant = point. Hence, `instant()` equals to `point()`. This object has getter and setter functions attached.

   Here are the references about everything is object in Python:

[Python 函数自定义属性_人气小姜的博客-CSDN博客_python自定义属性](https://blog.csdn.net/windyJ809/article/details/117071605)

[Python 里为什么函数可以返回一个函数内部定义的函数？ - 知乎](https://www.zhihu.com/question/25950466)

   Noted that if we only want to read the variable in the outer function, everything is ok. But if we want to set values to the variable in the outer function, Python will regard the variable in the equation as a local variable. Hence, we need to add `nonlocal` statement in the inner function to tell the inner function that variable *x* is not the local variable.

   Here is the reference about nonlocal:

[返回函数](https://www.liaoxuefeng.com/wiki/1016959663602400/1017434209254976)

1. Decorators

   Decorators are higher-order functions that take a callable (function, method, class) as an argument and return another callable. You can use decorator functions to add responsibilities to an existing callable dynamically and extend its behavior transparently without affecting or modifying the original callable.

```python
@decorator
def decorated_func():
    # Function body...
    pass
```

   This syntax makes decorator() automatically take decorated_func() as an argument and processes it in its body. This operation is a shorthand for the following assignment:

```python
decorated_func = decorator(decorated_func)
```

   Here’s an example of how to build a decorator function to add new functionality to an existing function:

```python
>>> def add_messages(func):
...     def _add_messages():
...         print("This is my first decorator")
...         func()
...         print("Bye!")
...     return _add_messages

>>> @add_messages
... def greet():
...     print("Hello, World!")

>>> greet()
This is my first decorator
Hello, World!
Bye!
```

   The use cases for Python decorators are varied. Here are some of them:

   - Debugging
   - Caching
   - Logging
   - Timing

   A common practice for debugging Python code is to insert calls to print() to check the values of variables, to confirm that a code block gets executed, and so on. Adding and removing calls to print() can be annoying, and you run the risk of forgetting some of them. To prevent this situation, you can write a decorator like this:

```python
>>> def debug(func):
...     def _debug(*args, **kwargs):
...         result = func(*args, **kwargs)
...         print(
...             f"{func.__name__}(args: {args}, kwargs: {kwargs}) -> {result}"
...         )
...         return result
...     return _debug

>>> @debug
... def add(a, b):
...     return a + b

>>> add(5, 6)
# add(args: (5, 6), kwargs: {}) -> 11
# 11
```

   Once you get the desired result, you can remove the decorator call @debug, and your function will ready for the next step.

   Here’s a final example of how to create a decorator. This time, you’ll reimplement generate_power() as a decorator function:

```python
>>> def generate_power(exponent):
...     def power(func):
...         def inner_power(*args):
...             base = func(*args)
...             return base ** exponent
...         return inner_power
...     return power
...

>>> @generate_power(2)
... def raise_two(n):
...     return n
...
>>> raise_two(7)
49

>>> @generate_power(3)
... def raise_three(n):
...     return n
...
>>> raise_three(5)
125
```

   In this case, you use both a closure to remember exponent and a decorator that returns a modified version of the input function, `func()`.

   Here, the decorator needs to take an argument (exponent), so you need to have two nested levels of inner functions. The first level is represented by `power()`, which takes the decorated function as an argument. The second level is represented by `inner_power()`, which packs the argument exponent in args, makes the final calculation of the power, and returns the result.

Here is the reference of Inner Function:

[Python Inner Functions: What Are They Good For? – Real Python](https://realpython.com/inner-functions-what-are-they-good-for/)

[装饰器](https://www.liaoxuefeng.com/wiki/1016959663602400/1017451662295584)

## Solution 1

```python
class Solution:
    def overlap(self, a, b):
        return a[0] <= b[1] and b[0] <= a[1]

    # generate graph where there is an undirected edge between intervals u
    # and v iff u and v overlap.
    def buildGraph(self, intervals):
        graph = collections.defaultdict(list)

        for i, interval_i in enumerate(intervals):
            for j in range(i+1, len(intervals)):
                if self.overlap(interval_i, intervals[j]):
                    graph[tuple(interval_i)].append(intervals[j])
                    graph[tuple(intervals[j])].append(interval_i)

        return graph

    # merges all of the nodes in this connected component into one interval.
    def mergeNodes(self, nodes):
        min_start = min(node[0] for node in nodes)
        max_end = max(node[1] for node in nodes)
        return [min_start, max_end]

    # gets the connected components of the interval overlap graph.
    def getComponents(self, graph, intervals):
        visited = set()
        comp_number = 0
        nodes_in_comp = collections.defaultdict(list)

        def markComponentDFS(start):
            stack = [start]
            while stack:
                node = tuple(stack.pop())
                if node not in visited:
                    visited.add(node)
                    nodes_in_comp[comp_number].append(node)
                    stack.extend(graph[node])

        # mark all nodes in the same connected component with the same integer.
        for interval in intervals:
            if tuple(interval) not in visited:
                markComponentDFS(interval)
                comp_number += 1

        return nodes_in_comp, comp_number


    def merge(self, intervals: List[List[int]]) -> List[List[int]]:
        graph = self.buildGraph(intervals)
        nodes_in_comp, number_of_comps = self.getComponents(graph, intervals)

        # all intervals in each connected component must be merged.
        return [self.mergeNodes(nodes_in_comp[comp]) for comp in range(number_of_comps)]
```

With the above intuition in mind, we can represent the graph as an adjacency list, inserting directed edges in both directions to simulate undirected edges. Then, to determine which connected component each node is it, we perform graph traversals from arbitrary unvisited nodes until all nodes have been visited. To do this efficiently, we store visited nodes in a Set, allowing for constant time containment checks and insertion. Finally, we consider each connected component, merging all of its intervals by constructing a new Interval with start equal to the minimum start among them and end equal to the maximum end.

The reason for the connected component search is that two intervals may not directly overlap, but might overlap indirectly via a third interval. See the example below to see this more clearly. The reason for the connected component search is that two intervals may not directly overlap, but might overlap indirectly via a third interval. See the example below to see this more clearly.

![Image.png](https://res.craft.do/user/full/171a925b-c816-75d3-6abe-c44832d05cd6/doc/69A46025-A6DE-40FD-8BB1-71D47D1A17B2/6BA3925C-64B4-431E-B5BA-A4EB8562105A_2/UUwz19hphIUrKGIxOcUC1ebPxheG3D209S0z8Mj2R9sz/Image.png)

There are 3 key-steps!

1. Build the Graph which includes that each interval’s neighbor.
2. Get the Component which we should use `while` until no more nodes can be found (`pop()`).
3. Merge the nodes in the same component whose `start` equal to the minimum start among them and `end` equal to the maximum end.

### Complexity Analysis

#### Time complexity : $$O(n^2)$$

Building the graph costs $$O(V + E) = O(V) + O(E) = O(n) + O(n^2) = O(n^2$$) time, as in the worst case all intervals are mutually overlapping. Traversing the graph has the same cost (although it might appear higher at first) because our visited set guarantees that each node will be visited exactly once. Finally, because each node is part of exactly one component, the merge step costs $$O(V) = O(n)$$ time. This all adds up as follows:

$$O(n^2) + O(n^2) + O(n) = O(n^2)$$

#### Space complexity : $$O(n^2)$$

As previously mentioned, in the worst case, all intervals are mutually overlapping, so there will be an edge for every pair of intervals. Therefore, the memory footprint is quadratic in the input size.

## Solution 2

```python
class Solution:
    def merge(self, intervals: List[List[int]]) -> List[List[int]]:

        intervals.sort(key=lambda x: x[0])

        merged = []
        for interval in intervals:
            # if the list of merged intervals is empty or if the current
            # interval does not overlap with the previous, simply append it.
            if not merged or merged[-1][1] < interval[0]:
                merged.append(interval)
            else:
            # otherwise, there is overlap, so we merge the current and previous
            # intervals.
                merged[-1][1] = max(merged[-1][1], interval[1])

        return merged
```

### Intuition

If we sort the intervals by their ::start:: value, then each set of intervals that can be merged will appear as a contiguous "run" in the sorted list.

### Algorithm

First, we sort the list as described. Then, we insert the first interval into our ::merged:: list and continue considering each interval in turn as follows: If the current interval begins after the previous interval ends, then they do not overlap and we can append the current interval to ::merged::. Otherwise, they do overlap, and we merge them by updating the ::end:: of the previous interval if it is less than the ::end:: of the current interval.

A simple proof by contradiction shows that this algorithm always produces the correct answer. First, suppose that the algorithm at some point fails to merge two intervals that should be merged. This would imply that there exists some triple of indices $$i$$, $$j$$, and $$k$$ in a list of intervals $$\text{ints}$$ such that $$i < j < k$$ and $$(\text{ints[i]}, \text{ints[k]})$$ can be merged, but neither $$(\text{ints[i]}, \text{ints[j]})$$ nor $$(\text{ints[j]}, \text{ints[k]})$$can be merged. From this scenario follow several inequalities:

$$
\text{ints[i].end} < \text{ints[j].start} \\ 
\text{ints[j].end} < \text{ints[k].start} \\
\text{ints[i].end} \geq \text{ints[k].start}
$$

We can chain these inequalities (along with the following inequality, implied by the well-formedness of the intervals: $$\text{ints[j].start} \leq \text{ints[j].end}$$) to demonstrate a contradiction:

$$
\text{ints[i].end} < \text{ints[j].start} \leq \text{ints[j].end} < \text{ints[k].start} \\ \text{ints[i].end} \geq \text{ints[k].start}
$$

### Complexity Analysis

#### Time complexity : $$O(n\log{}n)$$

Other than the ::sort:: invocation, we do a simple linear scan of the list, so the runtime is dominated by the $$O(n\log{}n)$$ complexity of sorting.

#### Space complexity : $$O(\log N)$$(or $$O(n)$$)

If we can sort ::intervals:: in place, we do not need more than constant additional space, although the sorting itself takes $$O(\log n)$$ space. Otherwise, we must allocate linear space to store a copy of ::intervals:: and sort that.

